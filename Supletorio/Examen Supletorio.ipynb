{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9b33b76701990f4",
   "metadata": {},
   "source": [
    "## Examen Final de Recuperación de Información\n",
    "\n",
    "_Modalidad:_ Práctico\n",
    "\n",
    "_Entrega:_ Jupyter Notebook con código y análisis\n",
    "\n",
    "_Valor:_ 20 puntos\n",
    "\n",
    "### Objetivo:\n",
    "\n",
    "Desarrollar un sistema de recuperación de información basado en un corpus de documentos. Se deben aplicar técnicas de preprocesamiento, indexación, representación en espacio vectorial y evaluación de tres métodos de recuperación de información mediante benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c763bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "from gensim.models import Word2Vec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from IPython.display import display\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "380d82f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\glenn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\glenn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\glenn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descargar recursos de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4050a76b130f12",
   "metadata": {},
   "source": [
    "## Parte 1: Selección y Preprocesamiento del Corpus (4 puntos)  \n",
    "Se trabajará con el corpus **20 Newsgroups**, un conjunto de documentos de texto extraídos de foros de discusión en diversas categorías. Se puede descargar con `sklearn.datasets.fetch_20newsgroups`.  \n",
    "\n",
    "1. **Carga del corpus** (1 punto): Descargar y visualizar ejemplos de textos.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fce0ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el corpus\n",
    "# categories = ['sci.space', 'rec.sport.baseball', 'talk.politics.mideast']  # Se pueden modificar\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ba70137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeros 5 documentos sin procesar:\n",
      "Documento 1: \n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n",
      "\n",
      "Documento 2: My brother is in the market for a high-performance video card that supports\n",
      "VESA local bus with 1-2MB RAM.  Does anyone have suggestions/ideas on:\n",
      "\n",
      "  - Diamond Stealth Pro Local Bus\n",
      "\n",
      "  - Orchid Farenheit 1280\n",
      "\n",
      "  - ATI Graphics Ultra Pro\n",
      "\n",
      "  - Any other high-performance VLB card\n",
      "\n",
      "\n",
      "Please post or email.  Thank you!\n",
      "\n",
      "  - Matt\n",
      "\n",
      "\n",
      "Documento 3: \n",
      "\n",
      "\n",
      "\n",
      "\tFinally you said what you dream about. Mediterranean???? That was new....\n",
      "\tThe area will be \"greater\" after some years, like your \"holocaust\" numbers......\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t*****\n",
      "\tIs't July in USA now????? Here in Sweden it's April and still cold.\n",
      "\tOr have you changed your calendar???\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t    ****************\n",
      "\t\t\t\t\t\t    ******************\n",
      "\t\t\t    ***************\n",
      "\n",
      "\n",
      "\tNOTHING OF THE MENTIONED IS TRUE, BUT LET SAY IT's TRUE.\n",
      "\t\n",
      "\tSHALL THE AZERI WOMEN AND CHILDREN GOING TO PAY THE PRICE WITH\n",
      "\t\t\t\t\t\t    **************\n",
      "\tBEING RAPED, KILLED AND TORTURED BY THE ARMENIANS??????????\n",
      "\t\n",
      "\tHAVE YOU HEARDED SOMETHING CALLED: \"GENEVA CONVENTION\"???????\n",
      "\tYOU FACIST!!!!!\n",
      "\n",
      "\n",
      "\n",
      "\tOhhh i forgot, this is how Armenians fight, nobody has forgot\n",
      "\tyou killings, rapings and torture against the Kurds and Turks once\n",
      "\tupon a time!\n",
      "      \n",
      "       \n",
      "\n",
      "\n",
      "Ohhhh so swedish RedCross workers do lie they too? What ever you say\n",
      "\"regional killer\", if you don't like the person then shoot him that's your policy.....l\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\ti\n",
      "\t\t\t\t\t\t\t\t\t\ti\n",
      "\t\t\t\t\t\t\t\t\t\ti\n",
      "\tConfused?????\t\t\t\t\t\t\t\ti\n",
      "\t\t\t\t\t\t\t\t\t\ti\n",
      "        Search Turkish planes? You don't know what you are talking about.\ti\n",
      "        Turkey's government has announced that it's giving weapons  <-----------i\n",
      "        to Azerbadjan since Armenia started to attack Azerbadjan\t\t\n",
      "        it self, not the Karabag province. So why search a plane for weapons\t\n",
      "        since it's content is announced to be weapons?   \n",
      "\n",
      "\tIf there is one that's confused then that's you! We have the right (and we do)\n",
      "\tto give weapons to the Azeris, since Armenians started the fight in Azerbadjan!\n",
      " \n",
      "\n",
      "\n",
      "\tShoot down with what? Armenian bread and butter? Or the arms and personel \n",
      "\tof the Russian army?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Documento 4: \n",
      "Think!\n",
      "\n",
      "It's the SCSI card doing the DMA transfers NOT the disks...\n",
      "\n",
      "The SCSI card can do DMA transfers containing data from any of the SCSI devices\n",
      "it is attached when it wants to.\n",
      "\n",
      "An important feature of SCSI is the ability to detach a device. This frees the\n",
      "SCSI bus for other devices. This is typically used in a multi-tasking OS to\n",
      "start transfers on several devices. While each device is seeking the data the\n",
      "bus is free for other commands and data transfers. When the devices are\n",
      "ready to transfer the data they can aquire the bus and send the data.\n",
      "\n",
      "On an IDE bus when you start a transfer the bus is busy until the disk has seeked\n",
      "the data and transfered it. This is typically a 10-20ms second lock out for other\n",
      "processes wanting the bus irrespective of transfer time.\n",
      "\n",
      "\n",
      "Documento 5: 1)    I have an old Jasmine drive which I cannot use with my new system.\n",
      " My understanding is that I have to upsate the driver with a more modern\n",
      "one in order to gain compatability with system 7.0.1.  does anyone know\n",
      "of an inexpensive program to do this?  ( I have seen formatters for <$20\n",
      "buit have no idea if they will work)\n",
      " \n",
      "2)     I have another ancient device, this one a tape drive for which\n",
      "the back utility freezes the system if I try to use it.  THe drive is a\n",
      "jasmine direct tape (bought used for $150 w/ 6 tapes, techmar\n",
      "mechanism).  Essentially I have the same question as above, anyone know\n",
      "of an inexpensive beckup utility I can use with system 7.0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar los primeros 5 documentos sin procesar\n",
    "print(\"Primeros 5 documentos sin procesar:\")\n",
    "for i, doc in enumerate(newsgroups.data[:5]):\n",
    "    print(f\"Documento {i+1}: {doc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcd664b",
   "metadata": {},
   "source": [
    "2. **Preprocesamiento** (3 puntos): Implementar tokenización, eliminación de stopwords, lematización y vectorización del texto con TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4be7d4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocesamiento de texto: tokenización, stopwords, lematización y limpieza.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61e9c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar preprocesamiento al corpus\n",
    "preprocessed_docs = [preprocess_text(doc) for doc in newsgroups.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404a92aceb5fc8e6",
   "metadata": {},
   "source": [
    "## Parte 2: Indexación y Representación Vectorial (4 puntos)  \n",
    "1. Construir una representación en **espacio vectorial** usando **TF-IDF** (2 puntos).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a71e77f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorización con TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_docs)\n",
    "tfidf_matrix = normalize(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca06881c",
   "metadata": {},
   "source": [
    "2. Implementar una estructura de indexación eficiente como **Elasticsearch**, **FAISS** o **ChromaDB** (2 puntos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a806de7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexión con Elasticsearch\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "index_name = \"newsgroups_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9afa3e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'newsgroups_index'})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear índice en Elasticsearch\n",
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "\n",
    "es.indices.create(index=index_name, body={\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"custom_analyzer\": {\n",
    "                    \"type\": \"standard\",\n",
    "                    \"stopwords\": \"_english_\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b76acf44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18846, [])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexar documentos en Elasticsearch\n",
    "def generate_actions():\n",
    "    for i, doc in enumerate(preprocessed_docs):\n",
    "        yield {\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": i,\n",
    "            \"_source\": {\"text\": doc}\n",
    "        }\n",
    "\n",
    "bulk(es, generate_actions())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac82cb3799a9457",
   "metadata": {},
   "source": [
    "## Parte 3: Aplicación de Técnicas de Recuperación de Información (6 puntos)  \n",
    "Implementar tres enfoques de recuperación de información y comparar su desempeño:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e63b3bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de consulta\n",
    "query = \"computer graphics and image processing\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c125235f",
   "metadata": {},
   "source": [
    "1. **Búsqueda exacta con modelo vectorial TF-IDF y similitud del coseno** (2 puntos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f884c021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de búsqueda con TF-IDF y similitud del coseno\n",
    "def search_tfidf(query, top_n=10):\n",
    "    query_vec = tfidf_vectorizer.transform([preprocess_text(query)])\n",
    "    similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "    top_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "    return [(i, newsgroups.data[i], similarities[i]) for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8125eda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Results:\n",
      "Result 1: (Score: 0.4521)\n",
      "\n",
      "I usually use \"Algorithms for graphics and image processing\" by\n",
      "Theodosios Pavlidis, but other people here got them same idea and now\n",
      "3 of 4 copies in the libraries have been stolen!\n",
      "\n",
      "Another reference is \"Digital Image Processing\" by Gonzalez and\n",
      "Wintz/Wood, which is widely available but a little expensive ($55\n",
      "here- I just checked today).\n",
      "\n",
      "Result 2: (Score: 0.4441)\n",
      "Archive-name: graphics/resources-list/part2\n",
      "Last-modified: 1993/04/17\n",
      "\n",
      "\n",
      "Computer Graphics Resource Listing : WEEKLY POSTING [ PART 2/3 ]\n",
      "===================================================\n",
      "Last Change : 17 April 1993\n",
      "\n",
      "\n",
      "14. Plotting packages\n",
      "=====================\n",
      "\n",
      "Gnuplot 3.2\n",
      "-----------\n",
      "  It is one of the best 2- and 3-D plotting packages, with\n",
      "  online help.It's a command-line driven interactive function plotting utility\n",
      "  for UNIX, MSDOS, Amiga, Archimedes, and VMS platforms (at least!).\n",
      "  Fre\n",
      "\n",
      "Result 3: (Score: 0.4428)\n",
      "Archive-name: graphics/resources-list/part2\n",
      "Last-modified: 1993/04/27\n",
      "\n",
      "\n",
      "Computer Graphics Resource Listing : WEEKLY POSTING [ PART 2/3 ]\n",
      "===================================================\n",
      "Last Change : 27 April 1993\n",
      "\n",
      "\n",
      "14. Plotting packages\n",
      "=====================\n",
      "\n",
      "Gnuplot 3.2\n",
      "-----------\n",
      "  It is one of the best 2- and 3-D plotting packages, with\n",
      "  online help.It's a command-line driven interactive function plotting utility\n",
      "  for UNIX, MSDOS, Amiga, Archimedes, and VMS platforms (at least!).\n",
      "  Fre\n",
      "\n",
      "Result 4: (Score: 0.4156)\n",
      "I'm interested in find out what is involved in processing pairs of \n",
      "stereo photographs.  I have black-and-white photos and would like \n",
      "to obtain surface contours.\n",
      "\n",
      "I'd prefer to do the processing on an SGI, but would be interested\n",
      "in hearing what software/hardware is used for this type of\n",
      "image processing.\n",
      "\n",
      "Please email and/or post to comp.sys.sgi.graphics your responses.\n",
      "\n",
      "Thanks,\n",
      "\n",
      "Result 5: (Score: 0.3993)\n",
      "This is not flame, or abuse, nor do I want to start another thread (this\n",
      "is, after all, supposed to be about IMAGE PROCESSING).\n",
      "\n",
      "Result 6: (Score: 0.3568)\n",
      "I am happy to announce the first public release of the bit program,\n",
      "   an INTERACTIVE, FULL COLOR image viewer and editor based on SGI GL.\n",
      "   Besides typical touchup tasks, such as crop, rotate, smooth, etc,\n",
      "   bit offers some unique features not available in similar programs,\n",
      "   such as text and vector support and the separation of text and image.\n",
      "\n",
      "   The following is the relevant sections from the man page. \n",
      "   -----------------------------------------------------------------\n",
      "\n",
      "Pre-Release of M\n",
      "\n",
      "Result 7: (Score: 0.3539)\n",
      "Technion - Israel Institute of Technology\n",
      "         Department of Computer Science\n",
      "\n",
      "       GRADUATE STUDIES IN COMPUTER GRAPHICS\n",
      "\n",
      "Applications are invited for graduate students wishing\n",
      "to specialize in computer graphics and related fields.\n",
      "Active research is being conducted in the fields of\n",
      "image rendering, geometric modelling and computer animation.\n",
      "State of the art graphics workstations (Sun, Silicon Graphics)\n",
      "and video equipment are available.\n",
      "The Technion offers full scholarship support (tuit\n",
      "\n",
      "Result 8: (Score: 0.3395)\n",
      "\n",
      "\tYes, that's known as \"Bresenhams Run Length Slice Algorithm for\n",
      "Incremental lines\". See Fundamental Algorithms for Computer Graphics,\n",
      "Springer-Verlag, Berlin Heidelberg 1985.\n",
      "\n",
      "\n",
      "\tHmm. I don't think I can help you with this, but you might\n",
      "take a look at the following:\n",
      "\n",
      "\t\"Double-Step Incremental Generation of Lines and Circles\",\n",
      "X. Wu and J. G. Rokne, Computer Graphics and Image processing,\n",
      "Vol 37, No. 4, Mar. 1987, pp. 331-334\n",
      "\n",
      "\t\"Double-Step Generation of Ellipses\", X. Wu and J. G. Rokne,\n",
      "IEEE C\n",
      "\n",
      "Result 9: (Score: 0.3386)\n",
      "Check out Image Pals v1.2 from U-Lead (until May, special $99 intro price,\n",
      "310-523-9393). It has the basic image processing tools for all major formats,\n",
      "does screen grabbing, and allows all your image files to be calalogged into\n",
      "a thumbnail database. It's great!\n",
      "\n",
      "\n",
      "Result 10: (Score: 0.3282)\n",
      "Appsoft Image is available for NeXTStep. It is a image processing program\n",
      "similar to Adobe Photoshop. It is reviewed in the April '93 issue of\n",
      "Publish! Magazine.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"TF-IDF Results:\")\n",
    "results_tfidf = search_tfidf(query)\n",
    "for idx, (doc_id, doc, score) in enumerate(results_tfidf):\n",
    "    print(f\"Result {idx+1}: (Score: {score:.4f})\\n{doc[:500]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e25859",
   "metadata": {},
   "source": [
    "2. **Búsqueda basada en Word2Vec** (2 puntos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8272b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo Word2Vec\n",
    "sentences = [doc.split() for doc in preprocessed_docs]\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a7ebb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_vector(words):\n",
    "    vectors = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "def search_word2vec(query, top_n=5):\n",
    "    query_tokens = preprocess_text(query).split()\n",
    "    query_vec = get_word2vec_vector(query_tokens)\n",
    "    similarities = []\n",
    "    \n",
    "    for doc in preprocessed_docs:\n",
    "        doc_tokens = doc.split()\n",
    "        doc_vec = get_word2vec_vector(doc_tokens)\n",
    "        similarity = np.dot(query_vec, doc_vec) if np.any(doc_vec) else 0\n",
    "        similarities.append(similarity)\n",
    "    \n",
    "    top_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "    return [(i, newsgroups.data[i], similarities[i]) for i in top_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e7c6d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word2Vec Results:\n",
      "Result 1: (Score: 61.0962)\n",
      " and  A VGA monitor..\n",
      "e-mail\n",
      "\n",
      "\n",
      "Result 2: (Score: 59.4233)\n",
      "Not on my system.\n",
      "\n",
      "Result 3: (Score: 54.6962)\n",
      "Which newsgroup discusses graphic design on PCs and macs?\n",
      "\n",
      "Result 4: (Score: 54.4217)\n",
      "\n",
      "Version 2.03 drivers are current.\n",
      "\n",
      "Result 5: (Score: 53.5735)\n",
      "\n",
      "No.  Plug the printer in the printer port, and the modem in the modem\n",
      "port. ;)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nWord2Vec Results:\")\n",
    "results_word2vec = search_word2vec(query)\n",
    "for idx, (doc_id,doc, score) in enumerate(results_word2vec):\n",
    "    print(f\"Result {idx+1}: (Score: {score:.4f})\\n{doc[:500]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04a9dd1",
   "metadata": {},
   "source": [
    "3. **Recuperación con un modelo basado en transformers (Ej: `sentence-transformers` para embeddings)** (2 puntos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6128150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo basado en transformers\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "X_sbert = sbert_model.encode(preprocessed_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1486d56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_sbert(query, top_n=10):\n",
    "    query_vec = sbert_model.encode([query])[0]\n",
    "    similarities = cosine_similarity([query_vec], X_sbert).flatten()\n",
    "    top_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "    return [(i, newsgroups.data[i], similarities[i]) for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a5088e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SBERT Results:\n",
      "Result 1: (Score: 0.5729)\n",
      "\n",
      "I usually use \"Algorithms for graphics and image processing\" by\n",
      "Theodosios Pavlidis, but other people here got them same idea and now\n",
      "3 of 4 copies in the libraries have been stolen!\n",
      "\n",
      "Another reference is \"Digital Image Processing\" by Gonzalez and\n",
      "Wintz/Wood, which is widely available but a little expensive ($55\n",
      "here- I just checked today).\n",
      "\n",
      "Result 2: (Score: 0.5593)\n",
      "Hello,\n",
      "\n",
      "    I am searching for rendering software which has been developed\n",
      "to specifically take advantage of multi-processor computer systems.\n",
      "Any pointers to such software would be greatly appreciated.\n",
      "    \n",
      "Thanks.\n",
      "\n",
      "\n",
      "Result 3: (Score: 0.5532)\n",
      "\n",
      "What kind of polygons?  Shaded?  Texturemapped?  Hm?  More comes into play with\n",
      "fast routines than just \"polygons\".  It would be nice to know exaclty what\n",
      "system (VGA is a start, but what processor?) and a few of the specifics of the\n",
      "implementation.  You need to give  more info if you want to get any answers! :P\n",
      "\n",
      "                                  - Ian Romanick\n",
      "                                    Dancing Fool of Epsilon\n",
      "\n",
      "Result 4: (Score: 0.5357)\n",
      "A shareware graphics program called Pman has a filter that makes a picture\n",
      "look like a hand drawing.  This picture could probably be converted into\n",
      "vector format much easier because it is all lines. (With Corel Trace, etc..)\n",
      "\n",
      "\n",
      "Result 5: (Score: 0.5304)\n",
      "\n",
      "A book that I can somewhat recommend is :\n",
      "                     \n",
      "                     Pratical Image Processing in C\n",
      "                     by Craig A. Lindley\n",
      "                     published by Wiley\n",
      "\n",
      "Result 6: (Score: 0.5238)\n",
      "I'm making a customized paint program in DOS and need an algorithm\n",
      "for reading bitmap files like GIF, PCX, or BMP.  Does anyone have\n",
      "such an algorithm?  I've tried copying one out of a book for reading\n",
      ".PCX format but it doesn't work.  I will take an algorithm for any\n",
      "format that can be created from Windows Paint.  \n",
      "Thanks!\n",
      "Toni\n",
      "\n",
      "\n",
      "Result 7: (Score: 0.5085)\n",
      "Hi\n",
      "\n",
      "I am looking for Image Analysis software running in DOS or Windows. I'd like \n",
      "to be able to analyze TIFF or similar files to generate histograms of \n",
      "patterns, etc. \n",
      "\n",
      "Any help would be appreciated!\n",
      "\n",
      "__________________________________________________________________________\n",
      "\n",
      "Result 8: (Score: 0.5085)\n",
      "Hello,\n",
      "\n",
      "    I have been searching for a quality image enhancement and\n",
      "    manipulation package for Unix/X/Motif platforms that is comparable\n",
      "    to Adobe Photo Shop for the Mac.\n",
      "\n",
      "    I have not been able to find any, and would appreciate any\n",
      "    information about such products you could provide.\n",
      "\n",
      "    I would be particularly interested in software that runs on HP or\n",
      "    Sun workstations, and does not require special add-in hardware, but\n",
      "    would also be interested in other solutions.\n",
      "\n",
      "\n",
      "Thank You\n",
      "\n",
      "Result 9: (Score: 0.4995)\n",
      "Technion - Israel Institute of Technology\n",
      "         Department of Computer Science\n",
      "\n",
      "       GRADUATE STUDIES IN COMPUTER GRAPHICS\n",
      "\n",
      "Applications are invited for graduate students wishing\n",
      "to specialize in computer graphics and related fields.\n",
      "Active research is being conducted in the fields of\n",
      "image rendering, geometric modelling and computer animation.\n",
      "State of the art graphics workstations (Sun, Silicon Graphics)\n",
      "and video equipment are available.\n",
      "The Technion offers full scholarship support (tuit\n",
      "\n",
      "Result 10: (Score: 0.4990)\n",
      "I saw an imaging program some time ago on an Amiga that had\n",
      "Cross, Sobel and Roberts filters for edge detection. \n",
      "\n",
      "Can anybody direct me to these algorithms.\n",
      "\n",
      "Paul Denize\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "Paul Denize                                Internet: PDenize@Waikato.ac.nz\n",
      "Department of Computer Science\n",
      "University of Waikato                         phone: ++64 7 8562-889\n",
      "Hamilton                                                          Ext 8743\n",
      "NEW\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSBERT Results:\")\n",
    "results_sbert = search_sbert(query)\n",
    "for idx, (doc_id,doc, score) in enumerate(results_sbert):\n",
    "    print(f\"Result {idx+1}: (Score: {score:.4f})\\n{doc[:500]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91974ad88e6a4870",
   "metadata": {},
   "source": [
    "## Parte 4: Evaluación mediante Benchmarking (6 puntos)  \n",
    "1. **Definición de una Ground Truth** (2 puntos): Se deben seleccionar al menos 10 consultas y definir manualmente los documentos relevantes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed90c502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from collections import defaultdict\n",
    "\n",
    "ground_truth_texts = {\n",
    "    \n",
    "    \"computer graphics and image processing\": [\"5776\",\"11244\",\"18102\",\"1152\",\"17029\"],\n",
    "    \"space mission\": [\"2056\",\"8668\",\"10867\",\"15147\",\"16936\"],\n",
    "    \"political debate\": [\"3438\",\"16274\",\"15562\",\"10114\",\"1126\"],\n",
    "    \"baseball game\": [\"6874\",\"15809\",\"5020\",\"5366\",\"2912\"],\n",
    "    \"climate change\": [\"7704\",\"9583\",\"8367\",\"17685\",\"4523\"],\n",
    "    \"quantum mechanics\": [\"7025\",\"6451\",\"13633\",\"6507\",\"7412\"],\n",
    "    \"artificial intelligence\": [\"3565\",\"171\",\"5170\",\"1319\",\"3415\"],\n",
    "    \"financial markets\": [\"12407\",\"14729\",\"8107\",\"15585\",\"5534\"],\n",
    "    \"healthcare technology\": [\"6017\",\"4246\",\"3076\",\"7895\",\"11880\"],\n",
    "    \"renewable energy\": [\"5217\",\"9695\",\"182\",\"4653\",\"11567\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8bb2903",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = {query: [int(idx) for idx in indices] for query, indices in ground_truth_texts.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d166362",
   "metadata": {},
   "source": [
    "2. **Cálculo de precisión y recall para cada técnica** (2 puntos): Implementar evaluación con métricas estándar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27f5b32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(search_function, model_name, top_n=10):\n",
    "    results = []\n",
    "    for query, relevant_indices in ground_truth.items():\n",
    "        print(f\"\\n**Evaluando:** {query} ({model_name})\")\n",
    "        retrieved = search_function(query, top_n=top_n)\n",
    "        retrieved_indices = [doc_id for doc_id, _, _ in retrieved]\n",
    "        \n",
    "        total_relevant = len(relevant_indices)\n",
    "        relevant_count = len(set(retrieved_indices) & set(relevant_indices))\n",
    "        \n",
    "        precision = relevant_count / top_n if top_n > 0 else 0\n",
    "        recall = relevant_count / total_relevant if total_relevant > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        print(f\"Relevant docs: {relevant_count} / {total_relevant}\")\n",
    "        print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "        results.append((query, precision, recall, f1))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8d6faa",
   "metadata": {},
   "source": [
    "3. **Análisis comparativo** (2 puntos): Comparar los resultados de las tres técnicas y justificar su efectividad con base en los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2794097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Evaluando:** computer graphics and image processing (TF-IDF)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant docs: 5 / 5\n",
      "Precision: 0.5000, Recall: 1.0000, F1: 0.6667\n",
      "\n",
      "**Evaluando:** space mission (TF-IDF)\n",
      "Relevant docs: 5 / 5\n",
      "Precision: 0.5000, Recall: 1.0000, F1: 0.6667\n",
      "\n",
      "**Evaluando:** political debate (TF-IDF)\n",
      "Relevant docs: 5 / 5\n",
      "Precision: 0.5000, Recall: 1.0000, F1: 0.6667\n",
      "\n",
      "**Evaluando:** baseball game (TF-IDF)\n",
      "Relevant docs: 5 / 5\n",
      "Precision: 0.5000, Recall: 1.0000, F1: 0.6667\n",
      "\n",
      "**Evaluando:** climate change (TF-IDF)\n",
      "Relevant docs: 5 / 5\n",
      "Precision: 0.5000, Recall: 1.0000, F1: 0.6667\n",
      "\n",
      "**Evaluando:** quantum mechanics (TF-IDF)\n",
      "Relevant docs: 5 / 5\n",
      "Precision: 0.5000, Recall: 1.0000, F1: 0.6667\n",
      "\n",
      "**Evaluando:** artificial intelligence (TF-IDF)\n",
      "Relevant docs: 5 / 5\n",
      "Precision: 0.5000, Recall: 1.0000, F1: 0.6667\n",
      "\n",
      "**Evaluando:** financial markets (TF-IDF)\n",
      "Relevant docs: 5 / 5\n",
      "Precision: 0.5000, Recall: 1.0000, F1: 0.6667\n",
      "\n",
      "**Evaluando:** healthcare technology (TF-IDF)\n",
      "Relevant docs: 5 / 5\n",
      "Precision: 0.5000, Recall: 1.0000, F1: 0.6667\n",
      "\n",
      "**Evaluando:** renewable energy (TF-IDF)\n",
      "Relevant docs: 5 / 5\n",
      "Precision: 0.5000, Recall: 1.0000, F1: 0.6667\n",
      "\n",
      "**Evaluando:** computer graphics and image processing (Word2Vec)\n",
      "Relevant docs: 0 / 5\n",
      "Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "\n",
      "**Evaluando:** space mission (Word2Vec)\n",
      "Relevant docs: 0 / 5\n",
      "Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "\n",
      "**Evaluando:** political debate (Word2Vec)\n",
      "Relevant docs: 0 / 5\n",
      "Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "\n",
      "**Evaluando:** baseball game (Word2Vec)\n",
      "Relevant docs: 1 / 5\n",
      "Precision: 0.1000, Recall: 0.2000, F1: 0.1333\n",
      "\n",
      "**Evaluando:** climate change (Word2Vec)\n",
      "Relevant docs: 0 / 5\n",
      "Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "\n",
      "**Evaluando:** quantum mechanics (Word2Vec)\n",
      "Relevant docs: 0 / 5\n",
      "Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "\n",
      "**Evaluando:** artificial intelligence (Word2Vec)\n",
      "Relevant docs: 0 / 5\n",
      "Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "\n",
      "**Evaluando:** financial markets (Word2Vec)\n",
      "Relevant docs: 1 / 5\n",
      "Precision: 0.1000, Recall: 0.2000, F1: 0.1333\n",
      "\n",
      "**Evaluando:** healthcare technology (Word2Vec)\n",
      "Relevant docs: 0 / 5\n",
      "Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "\n",
      "**Evaluando:** renewable energy (Word2Vec)\n",
      "Relevant docs: 0 / 5\n",
      "Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "\n",
      "**Evaluando:** computer graphics and image processing (SBERT)\n",
      "Relevant docs: 1 / 5\n",
      "Precision: 0.1000, Recall: 0.2000, F1: 0.1333\n",
      "\n",
      "**Evaluando:** space mission (SBERT)\n",
      "Relevant docs: 0 / 5\n",
      "Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "\n",
      "**Evaluando:** political debate (SBERT)\n",
      "Relevant docs: 1 / 5\n",
      "Precision: 0.1000, Recall: 0.2000, F1: 0.1333\n",
      "\n",
      "**Evaluando:** baseball game (SBERT)\n",
      "Relevant docs: 2 / 5\n",
      "Precision: 0.2000, Recall: 0.4000, F1: 0.2667\n",
      "\n",
      "**Evaluando:** climate change (SBERT)\n",
      "Relevant docs: 1 / 5\n",
      "Precision: 0.1000, Recall: 0.2000, F1: 0.1333\n",
      "\n",
      "**Evaluando:** quantum mechanics (SBERT)\n",
      "Relevant docs: 3 / 5\n",
      "Precision: 0.3000, Recall: 0.6000, F1: 0.4000\n",
      "\n",
      "**Evaluando:** artificial intelligence (SBERT)\n",
      "Relevant docs: 1 / 5\n",
      "Precision: 0.1000, Recall: 0.2000, F1: 0.1333\n",
      "\n",
      "**Evaluando:** financial markets (SBERT)\n",
      "Relevant docs: 1 / 5\n",
      "Precision: 0.1000, Recall: 0.2000, F1: 0.1333\n",
      "\n",
      "**Evaluando:** healthcare technology (SBERT)\n",
      "Relevant docs: 0 / 5\n",
      "Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "\n",
      "**Evaluando:** renewable energy (SBERT)\n",
      "Relevant docs: 2 / 5\n",
      "Precision: 0.2000, Recall: 0.4000, F1: 0.2667\n"
     ]
    }
   ],
   "source": [
    "techniques = {\n",
    "    \"TF-IDF\": search_tfidf,\n",
    "    \"Word2Vec\": search_word2vec,\n",
    "    \"SBERT\": search_sbert\n",
    "}\n",
    "\n",
    "results = {name: evaluate_model(func, name) for name, func in techniques.items()}\n",
    "\n",
    "df_results = pd.DataFrame(results[\"TF-IDF\"], columns=[\"Query\", \"Precision (TF-IDF)\", \"Recall (TF-IDF)\", \"F1-score (TF-IDF)\"])\n",
    "df_results[\"Precision (Word2Vec)\"], df_results[\"Recall (Word2Vec)\"], df_results[\"F1-score (Word2Vec)\"] = zip(*[(p, r, f) for _, p, r, f in results[\"Word2Vec\"]])\n",
    "df_results[\"Precision (SBERT)\"], df_results[\"Recall (SBERT)\"], df_results[\"F1-score (SBERT)\"] = zip(*[(p, r, f) for _, p, r, f in results[\"SBERT\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6cf3b2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar los resultados\n",
    "df_results.to_csv(\"benchmarking_results_text_based.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e350f5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Evaluación Final (Benchmarking basado en contenido):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Precision (TF-IDF)</th>\n",
       "      <th>Recall (TF-IDF)</th>\n",
       "      <th>F1-score (TF-IDF)</th>\n",
       "      <th>Precision (Word2Vec)</th>\n",
       "      <th>Recall (Word2Vec)</th>\n",
       "      <th>F1-score (Word2Vec)</th>\n",
       "      <th>Precision (SBERT)</th>\n",
       "      <th>Recall (SBERT)</th>\n",
       "      <th>F1-score (SBERT)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>computer graphics and image processing</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>space mission</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>political debate</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>baseball game</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>climate change</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>quantum mechanics</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>artificial intelligence</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>financial markets</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>healthcare technology</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>renewable energy</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Query  Precision (TF-IDF)  \\\n",
       "0  computer graphics and image processing                 0.5   \n",
       "1                           space mission                 0.5   \n",
       "2                        political debate                 0.5   \n",
       "3                           baseball game                 0.5   \n",
       "4                          climate change                 0.5   \n",
       "5                       quantum mechanics                 0.5   \n",
       "6                 artificial intelligence                 0.5   \n",
       "7                       financial markets                 0.5   \n",
       "8                   healthcare technology                 0.5   \n",
       "9                        renewable energy                 0.5   \n",
       "\n",
       "   Recall (TF-IDF)  F1-score (TF-IDF)  Precision (Word2Vec)  \\\n",
       "0              1.0           0.666667                   0.0   \n",
       "1              1.0           0.666667                   0.0   \n",
       "2              1.0           0.666667                   0.0   \n",
       "3              1.0           0.666667                   0.1   \n",
       "4              1.0           0.666667                   0.0   \n",
       "5              1.0           0.666667                   0.0   \n",
       "6              1.0           0.666667                   0.0   \n",
       "7              1.0           0.666667                   0.1   \n",
       "8              1.0           0.666667                   0.0   \n",
       "9              1.0           0.666667                   0.0   \n",
       "\n",
       "   Recall (Word2Vec)  F1-score (Word2Vec)  Precision (SBERT)  Recall (SBERT)  \\\n",
       "0                0.0             0.000000                0.1             0.2   \n",
       "1                0.0             0.000000                0.0             0.0   \n",
       "2                0.0             0.000000                0.1             0.2   \n",
       "3                0.2             0.133333                0.2             0.4   \n",
       "4                0.0             0.000000                0.1             0.2   \n",
       "5                0.0             0.000000                0.3             0.6   \n",
       "6                0.0             0.000000                0.1             0.2   \n",
       "7                0.2             0.133333                0.1             0.2   \n",
       "8                0.0             0.000000                0.0             0.0   \n",
       "9                0.0             0.000000                0.2             0.4   \n",
       "\n",
       "   F1-score (SBERT)  \n",
       "0          0.133333  \n",
       "1          0.000000  \n",
       "2          0.133333  \n",
       "3          0.266667  \n",
       "4          0.133333  \n",
       "5          0.400000  \n",
       "6          0.133333  \n",
       "7          0.133333  \n",
       "8          0.000000  \n",
       "9          0.266667  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n✅ Evaluación Final (Benchmarking basado en contenido):\")\n",
    "display(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762a5ad4",
   "metadata": {},
   "source": [
    "TF-IDF:\n",
    "\n",
    "- Presenta una precisión constante de 0.5 en todas las consultas.\n",
    "- Su recall es 1.0, lo que significa que recupera todos los documentos relevantes, pero también podría estar incluyendo documentos no relevantes.\n",
    "- Su F1-score de 0.66 indica un equilibrio moderado entre precisión y recall, pero su precisión relativamente baja sugiere que recupera demasiados documentos irrelevantes.\n",
    "\n",
    "Word2Vec:\n",
    "\n",
    "- Su desempeño es bastante bajo en comparación con los otros métodos.\n",
    "- En muchas consultas, su precisión y recall son 0.0, lo que indica que en esos casos no encontró documentos relevantes.\n",
    "- Cuando sí encuentra documentos relevantes, su F1-score sigue siendo bajo (0.13 en algunos casos), lo que sugiere que la representación semántica de Word2Vec no se adapta bien a este conjunto de datos.\n",
    "\n",
    "SBERT (Sentence-BERT):\n",
    "\n",
    "- Tiene una precisión variable, oscilando entre 0.0 y 0.3.\n",
    "- Su recall también varía entre 0.0 y 0.6, indicando que en algunos casos logra recuperar documentos relevantes, pero en otros no.\n",
    "- Su F1-score es mejor que Word2Vec, pero aún no supera a TF-IDF en la mayoría de los casos.\n",
    "- SBERT funciona mejor en algunas consultas como \"quantum mechanics\" (F1-score = 0.4), lo que indica que en ciertos temas captura mejor el significado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17abcfdd",
   "metadata": {},
   "source": [
    "TF-IDF sigue siendo la mejor opción en este benchmark, pues su recall es 1.0 y su F1-score es el más consistente (0.66 en todos los casos). Sin embargo, su precisión es relativamente baja, lo que indica que podría recuperar demasiados resultados irrelevantes. Word2Vec tiene un desempeño deficiente, probablemente porque el modelo no está bien entrenado para capturar el significado en este dominio o porque el corpus es pequeño, por lo cual puede que sea necesario más información para un mejor entrenamiento y así lograr mejores resultados. SBERT muestra mejoras con respecto a Word2Vec, pero aún no supera a TF-IDF en general. Sin embargo, en consultas específicas, logra un mejor balance entre precisión y recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa788a12e232a324",
   "metadata": {},
   "source": [
    "## Entrega y Formato  \n",
    "Cada estudiante debe entregar un **Jupyter Notebook** con el código y análisis bien documentado. Se evaluará claridad, calidad del código y profundidad del análisis.\n",
    "\n",
    "## Criterios de Evaluación (20 puntos)  \n",
    "| Sección | Puntos |\n",
    "|---------|--------|\n",
    "| Carga y preprocesamiento del corpus | 4 |\n",
    "| Indexación y representación vectorial | 4 |\n",
    "| Implementación de tres técnicas de recuperación | 6 |\n",
    "| Evaluación con benchmarking (ground truth, precisión y recall) | 6 |\n",
    "| **Total** | **20** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221979ab529e8f73",
   "metadata": {},
   "source": [
    "## Observaciones  \n",
    "- Se recomienda utilizar `scikit-learn`, `nltk`, `gensim`, `sentence-transformers` y `faiss` para la implementación.  \n",
    "- Se valorará la optimización del código y la presentación clara de los resultados.  \n",
    "- No se aceptan notebooks con errores de ejecución. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
